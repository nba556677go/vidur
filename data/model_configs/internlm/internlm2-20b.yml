num_layers: 48
num_q_heads: 48
num_kv_heads: 8
embedding_dim: 6144
mlp_hidden_dim: 16384
max_position_embeddings: 32768
use_gated_mlp: true
use_bias: false
use_qkv_bias: false
act: silu
norm: rms_norm
post_attn_norm: true
rope_scaling: null
rope_theta: 1000000
vocab_size: 92544
