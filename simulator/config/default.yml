seed: 42
log_level: info
output_dir: ./simulator_output/
cache_dir: ./cache
write_json_trace: false
write_chrome_trace: true
write_metrics: true

cluster:
  num_replicas: 1

replica:
  block_size: 16
  memory_margin_fraction: 0.1
  num_pipeline_stages: 4
  num_tensor_parallel_workers: 1
  model_name: meta-llama/Llama-2-7b-hf
  device: a100
  network_device: a100_pair_nvlink

request_generator:
  provider: synthetic
  max_tokens: 4096

synthetic_request_generator:
  length_provider: trace
  interval_provider: static
  min_tokens: 1024
  prefill_to_decode_ratio: 10
  num_requests: 128

trace_request_generator:
  trace_file: ./data/processed_traces/sydney_enterprise.csv
  date: '2023-08-21'
  prefill_scale_factor: 0.3
  decode_scale_factor:  1
  time_scale_factor: 0.04

# Config for synthetic trace generator
trace_request_length_generator:
  trace_file: ./data/processed_traces/lmsys_chat_1m_conversation_stats_llama2_tokenizer.csv
  prefill_scale_factor: 1
  decode_scale_factor:  1

trace_request_interval_generator:
  trace_file: ./data/processed_traces/AzureFunctionsInvocationTraceForTwoWeeksJan2021Processed.csv
  start_time: "1970-01-04 12:00:00"
  end_time: "1970-01-04 15:00:00"
  time_scale_factor: 0.3

poisson_request_interval_generator:
  qps: 0.5

gamma_request_interval_generator:
  cv: 0.5
  qps: 0.2

zipf_request_length_generator:
  theta: 0.4
  scramble: false

fixed_request_generator:
  prefill_tokens: 2048
  decode_tokens: 512

execution_time_predictor:
  provider: random_forrest
  # provider: linear_regression

sklearn_execution_time_predictor:
  compute_input_file: ./data/profiling/compute/{DEVICE}/{MODEL}/mlp.csv
  attention_input_file: ./data/profiling/compute/{DEVICE}/{MODEL}/attention.csv
  all_reduce_input_file: ./data/profiling/network/{NETWORK_DEVICE}/all_reduce.csv
  send_recv_input_file: ./data/profiling/network/{NETWORK_DEVICE}/send_recv.csv
  cpu_overhead_input_file: ./data/profiling/cpu_overhead/{NETWORK_DEVICE}/{MODEL}/cpu_overheads.csv
  k_fold_cv_splits: 10
  no_cache: false
  kv_cache_prediction_granularity: 64
  prediction_max_prefill_chunk_size: 4096
  prediction_max_batch_size: 128
  prediction_max_tokens_per_request: 4096
  attention_decode_batching_overhead_fraction: 0.1
  attention_prefill_batching_overhead_fraction: 0.1
  nccl_cpu_launch_overhead_ms: 0.020
  nccl_cpu_skew_overhead_per_device_ms: 0.0
  num_training_job_threads: -1
  skip_cpu_overhead_modeling: true

random_forrest_execution_time_predictor:
  num_estimators:
    # - 250
    - 500
    - 750
  max_depth:
    # - 8
    # - 16
    - 32
  min_samples_split:
    - 2
    - 5
    - 10

linear_regression_execution_time_predictor:
  polynomial_degree:
    - 1
    - 2
    - 3
    - 4
    - 5
  polynomial_include_bias:
    - true
    - false
  polynomial_interaction_only:
    - true
    - false
  fit_intercept:
    - true
    - false

simulator:
  time_limit: 0 # in seconds, 0 is no limit

global_scheduler:
  provider: round_robin

replica_scheduler:
  provider: sarathi
  batch_size_cap: 128
  num_blocks: null

orca_scheduler:
  use_single_prefill_per_batch: false

vllm_scheduler:
  watermark_blocks_fraction: 0.01
  max_tokens_in_batch: 4096

sarathi_scheduler:
  chunk_size: 512
  enable_rolling_prefills: true
  prefill_fitting_tolerance: 0.0
  watermark_blocks_fraction: 0.01

lightllm_scheduler:
  max_tokens_in_batch: 4096
  max_waiting_iters: 10

metrics_store:
  wandb_project: "llm-simulator-v2"
  wandb_group: ""
  wandb_run_name: ""
  subsamples: null
  save_table_to_wandb: false
  store_plots: true
  store_operation_metrics: false
  store_token_completion_metrics: false
  store_request_metrics: true
  store_batch_metrics: true
  store_utilization_metrics: true
  keep_individual_batch_metrics: false
  # min_batch_idx: 2000
  # max_batch_idx: 5000
